{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da23f271-f965-4294-901a-98d8b93a75eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from numpy import zeros, ones\n",
    "from numpy.random import randn, randint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Conv2DTranspose, LeakyReLU\n",
    "from keras.layers import BatchNormalization, Dropout, Reshape, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce8c855-2d24-4bd8-be74-75ff3d94ee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"/kaggle/input/50k-celebrity-faces-image-dataset/Celebrity_Faces_Dataset\" \n",
    "image_paths = glob.glob(os.path.join(dataset_dir, '*.jpg'))\n",
    "image_paths = image_paths[:20000]\n",
    "def load_and_preprocess_real_images(image_path, target_size=(64, 64)):\n",
    "    img = Image.open(image_path)\n",
    "    img = img.crop((0, 20, 178, 198))\n",
    "    img = img.resize(target_size)\n",
    "    img = np.array(img)/127.5 - 1\n",
    "    return img\n",
    "dataset = np.array([load_and_preprocess_real_images(img_path) for img_path in image_paths])\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22b475e-b6d6-456c-9146-f67c067293ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(6, 6, figsize=(15, 16))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = dataset[i]\n",
    "    img_rescaled = (img + 1) / 2\n",
    "    ax.imshow(img_rescaled)\n",
    "    ax.axis('off')\n",
    "fig.suptitle('Original Dataset Preprocessed Images', fontsize=25)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a109e16-a17c-4c32-be86-3a4a68ee5cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(image_shape=(64, 64, 3)):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(128, (3, 3), strides=(2,2), padding='same', input_shape=image_shape))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Conv2D(128, (3, 3), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Conv2D(256, (3, 3), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Conv2D(256, (3, 3), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Conv2D(512, (3, 3), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    optimizer = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "discriminator = build_discriminator()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a8c281-e363-4d82-b61c-998610c1e3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(latent_dim, channels=3):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16 * 16 * 128, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Reshape((16, 16, 128)))\n",
    "    model.add(Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Conv2DTranspose(128, (4, 4), strides=(1, 1), padding='same'))\n",
    "    model.add(LeakyReLU(0.2))  \n",
    "    model.add(Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(0.2))  \n",
    "    model.add(Conv2DTranspose(64, (4, 4), strides=(1, 1), padding='same'))\n",
    "    model.add(LeakyReLU(0.2))  \n",
    "    model.add(Conv2D(channels, (8, 8), activation='tanh', padding='same'))\n",
    "    return model\n",
    "generator = build_generator(100)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bde134-0e62-474d-8508-2b48b3cb89d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    optimizer = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51576882-4faf-47ba-9283-64cd28ee4664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_samples(dataset, num_samples):\n",
    "    sample_indices = randint(0, dataset.shape[0], num_samples)\n",
    "    X = dataset[sample_indices]\n",
    "    y = ones((num_samples, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71cccf6-826d-4267-ac66-1af7dee5b5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noise_samples(num_samples, noise_dim):\n",
    "    X_noise = randn(noise_dim * num_samples)\n",
    "    X_noise = X_noise.reshape(num_samples, noise_dim)\n",
    "    return X_noise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b09b068-bb85-405e-97e3-545ae1939df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_samples(generator, noise_dim, num_samples):\n",
    "    X_noise = generate_noise_samples(num_samples, noise_dim)\n",
    "    X = generator.predict(X_noise)\n",
    "    y = zeros((num_samples, 1 ))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5fb68f-0d64-472d-875a-3e0324d552d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(epoch, generator, num_samples=6, noise_dim=100):\n",
    "    X_noise = generate_noise_samples(num_samples, noise_dim)\n",
    "    X = generator.predict(X_noise, verbose=0)\n",
    "    X = (X + 1) / 2\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bf758e-122f-4dea-b944-49054361b1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_saved_images(saved_images, display_frequency):\n",
    "    for epoch, images in enumerate(saved_images):\n",
    "        fig, axes = plt.subplots(1, len(images), figsize=(15, 3))\n",
    "        for i, img in enumerate(images):\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].axis('off')\n",
    "        fig.suptitle(f\"Generated Images at Epoch {epoch*display_frequency + 1}\", fontsize=22)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30d3462-1cf1-4229-857c-f2e8f08c7cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_generated_images(epoch, generator, num_samples=6, noise_dim=100, figsize=(15, 3)):\n",
    "    X_noise = generate_noise_samples(num_samples, noise_dim)\n",
    "    X = generator.predict(X_noise, verbose=0)\n",
    "    X = (X + 1) / 2\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=figsize)\n",
    "    for i in range(num_samples):\n",
    "        axes[i].imshow(X[i])\n",
    "        axes[i].axis('off')\n",
    "    fig.suptitle(f\"Generated Images at Epoch {epoch+1}\", fontsize=22)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8684706-ada1-48e6-87f6-eacc96476103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(generator_model, discriminator_model, gan_model, dataset, noise_dimension,\n",
    "          num_epochs=100, batch_size=128, display_frequency=10, verbose=1):\n",
    "    \n",
    "    saved_images_for_epochs = []\n",
    "    \n",
    "    batches_per_epoch = int(dataset.shape[0] / batch_size)\n",
    "    \n",
    "    half_batch_size   = int(batch_size / 2)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_num in range(batches_per_epoch):\n",
    "            \n",
    "            real_images, real_labels    = generate_real_samples(dataset, half_batch_size)\n",
    "            dsr_loss_real, dsr_acc_real = discriminator_model.train_on_batch(real_images, real_labels)\n",
    "\n",
    "\n",
    "            fake_images, fake_labels    = generate_fake_samples(generator_model, noise_dimension, half_batch_size)\n",
    "            dsr_loss_fake, dsr_acc_fake = discriminator_model.train_on_batch(fake_images, fake_labels)\n",
    "            \n",
    "\n",
    "            dsr_loss = 0.5 * np.add(dsr_loss_real, dsr_loss_fake)\n",
    "            dsr_acc  = 0.5 * np.add(dsr_acc_real, dsr_acc_fake)\n",
    "            \n",
    "\n",
    "            gan_noise  = generate_noise_samples(batch_size, noise_dimension)\n",
    "            gan_labels = np.ones((batch_size, 1))\n",
    "            \n",
    "\n",
    "            gen_loss, _ = gan_model.train_on_batch(gan_noise, gan_labels)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"[ Epoch: {epoch+1} , Batch: {batch_num+1} ] --> [ Discriminator Loss : {dsr_loss:.6f} , Discriminator Accuracy: {100*dsr_acc:.2f}% ] [ Generator Loss: {gen_loss:.6f} ]\")\n",
    "                     \n",
    "\n",
    "        if epoch % display_frequency == 0:\n",
    "            generated_images_for_epoch = generate_images(epoch, generator_model)\n",
    "            saved_images_for_epochs.append(generated_images_for_epoch)\n",
    "            \n",
    "            # Plot generated images to visualize the progress of the generator\n",
    "            plot_generated_images(epoch, generator_model)\n",
    "\n",
    "    return saved_images_for_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de41ede-d7b0-4781-a7d3-bd3b3fee6b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dimension = 100\n",
    "discriminator = build_discriminator()\n",
    "generator = build_generator(noise_dimension)\n",
    "gan_model = build_gan(generator, discriminator)\n",
    "saved_images = train(generator, discriminator, gan_model, dataset, noise_dimension, num_epochs=251, batch_size=128, display_frequency=50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c23de6d-ff85-4c4c-a518-beb6a2057f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_saved_images(saved_images, display_frequency=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7046b12-884e-400b-afd8-c027fecea08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_generated_images_after_training(generator, noise_dim=100, figsize=(15, 16)):\n",
    "    \n",
    "    fig, axes = plt.subplots(6, 6, figsize=figsize)\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        X_noise = generate_noise_samples(1, noise_dim)\n",
    "        X = generator.predict(X_noise, verbose=0)\n",
    "        X = (X + 1) / 2\n",
    "        ax.imshow(X[0])\n",
    "        ax.axis('off')\n",
    "    fig.suptitle('Generated Images after Training for 250 Epochs', fontsize=25)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plot_generated_images_after_training(generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
